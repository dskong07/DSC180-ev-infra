{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if not using the api to gather data, refer to the next markdown entry for local dataset usage, and skip this section entirely. Only one or the other is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   https://data.ca.gov/dataset/vehicle-fuel-type-count-by-zip-code\n",
    "#   the resource IDS for each year can be found within the CKAN API documentation for each year. \n",
    "#   for this code to executed correctly, the REDACTED values can be replaced with the resource IDs corresponding to each year.\n",
    "#   the resource IDS follow the convention of the regex expression \"\\w{8}\\-\\w{4}\\-\\w{4}\\-\\w{4}\\-\\w{12}\"\n",
    "#   ALTERNATIVELY, you can download the datasets, and ignore this api portion, \n",
    "#   instead run the commented cell referring to local file acquisition.\n",
    "\n",
    "resource_ids = {2024:'TODO',2023:'TODO',2022:'TODO',\n",
    "                2021:'TODO',2020:'TODO',2019:'TODO'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.ca.gov/api/3/action/datastore_search?limit=10&resource_id=<TODO>'\n",
    "with requests.get(url) as response:\n",
    "        data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(resource_id, limit=10000):\n",
    "\n",
    "    #gets all entries from given resource id\n",
    "\n",
    "    url_template_base = 'https://data.ca.gov'\n",
    "    url_template = f'https://data.ca.gov/api/3/action/datastore_search?limit={limit}&resource_id='\n",
    "\n",
    "    d = []\n",
    "    rows = 0\n",
    "    next = False\n",
    "    nextkey = ''\n",
    "    url = url_template + resource_id\n",
    "\n",
    "    with requests.get(url) as response:\n",
    "        data = response.json()\n",
    "        rows = rows + len(data['result']['records'])\n",
    "        d.append(data)\n",
    "        print(rows, 'rows collected')\n",
    "\n",
    "    if data['result']['_links']['next'] != None:\n",
    "        next = True\n",
    "        nextkey = url_template_base + data['result']['_links']['next']\n",
    "\n",
    "    while next:\n",
    "\n",
    "        with requests.get(nextkey) as response:\n",
    "            data = response.json()\n",
    "            rows = rows + len(data['result']['records'])\n",
    "            d.append(data)\n",
    "\n",
    "        if len(data['result']['records']) < limit:\n",
    "            print(rows, 'rows collected total')\n",
    "            next = False\n",
    "        else:\n",
    "            print(rows, 'rows collected')\n",
    "            nextkey = url_template_base+data['result']['_links']['next']\n",
    "    return d\n",
    "    \n",
    "\n",
    "\n",
    "def format_dataset_list(d):\n",
    "    \n",
    "    #formats get_dataset output to list format\n",
    "\n",
    "    new_d = []\n",
    "    for i in range(len(d)):\n",
    "        data = d[i]['result']['records']\n",
    "        new_d = new_d + data\n",
    "    return {'results':new_d}\n",
    "\n",
    "\n",
    "\n",
    "def dataset_to_df(d):\n",
    "    ret = pd.DataFrame(d['results'])\n",
    "    ret['Vehicles'] = ret['Vehicles'].astype('int')\n",
    "    ret.columns = ['year_id', 'date', 'zipcode', 'modelyr', 'fuel', 'make', 'duty',\n",
    "       'num_vehicles']\n",
    "    ret['date'] = pd.to_datetime(ret['date'])\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(resource_ids,limit = 50000, path='datasets/vehicle_fuel_types/', save_output=False):\n",
    "    ret_list = []\n",
    "    for key in resource_ids:\n",
    "        print(f'working year {key}, resource_id {resource_ids[key]}')\n",
    "        dataset = get_dataset(resource_ids[key], limit)\n",
    "        d_list = format_dataset_list(dataset)\n",
    "        df = dataset_to_df(d_list)\n",
    "        if save_output:\n",
    "            df.to_csv(f'{path}{key}_vehicle_fuel_types.csv')\n",
    "            print(f'{key} dataset saved to {path}{key}_vehicle_fuel_types.csv')\n",
    "        ret_list.append(df)\n",
    "        print(f'year {key} idx={len(ret_list)-1} in ret\\n')\n",
    "    return ret_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save the datasets locally, change save_output to True\n",
    "# if planning to use the API rather than the direct downloads, set to True, so the files can be used with regards to\n",
    "# the datamerge and distributions notebook.\n",
    "veh_dfs = pipeline(resource_ids,save_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !! Local file dataset usage !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if rather than using the api, the files are downloaded directly from the source, uncomment this code without running above code.\n",
    "# replace the parameter with the local path of the relevant dataset path.\n",
    "\n",
    "\"\"\"\n",
    "column_names = ['year_id', 'date', 'zipcode', 'modelyr', 'fuel', 'make', 'duty',\n",
    "       'num_vehicles']\n",
    "\n",
    "df2019 = pd.read_csv('REPLACE WITH LOCAL 2019 DATASET PATH', dtype={'Zip Code':str, 'Model Year': str,'Vehicles': int})\n",
    "df2020 = pd.read_csv('REPLACE WITH LOCAL 2020 DATASET PATH', dtype={'Zip Code':str, 'Model Year': str,'Vehicles': int})\n",
    "df2021 = pd.read_csv('REPLACE WITH LOCAL 2021 DATASET PATH', dtype={'Zip Code':str, 'Model Year': str,'Vehicles': int})\n",
    "df2022 = pd.read_csv('REPLACE WITH LOCAL 2022 DATASET PATH', dtype={'Zip Code':str, 'Model Year': str,'Vehicles': int})\n",
    "df2023 = pd.read_csv('REPLACE WITH LOCAL 2023 DATASET PATH', dtype={'Zip Code':str, 'Model Year': str,'Vehicles': int})\n",
    "df2024 = pd.read_csv('REPLACE WITH LOCAL 2024 DATASET PATH', dtype={'Zip Code':str, 'Model Year': str,'Vehicles': int})\n",
    "veh_dfs = [df2019,df2020,df2021,df2022,df2023,df2024]\n",
    "veh_dfs.reverse()\n",
    "\n",
    "for df_i in veh_dfs:\n",
    "    df_i.columns = column_names\n",
    "\n",
    "column_names = ['date', 'zipcode', 'modelyr', 'fuel', 'make', 'duty',\n",
    "       'num_vehicles']\n",
    "for df_i in veh_dfs:\n",
    "    df_i.columns = column_names\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(veh_dfs)\n",
    "_24 = veh_dfs[0]\n",
    "_23 = veh_dfs[1]\n",
    "_22 = veh_dfs[2]\n",
    "_21 = veh_dfs[3]\n",
    "_20 = veh_dfs[4]\n",
    "_19 = veh_dfs[5]\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuels = df.fuel.unique()\n",
    "\n",
    "fuels\n",
    "def get_classes(df):\n",
    "    fuels = df.fuel.unique()\n",
    "    makes = df.make.unique()\n",
    "    duties = df.duty.unique()\n",
    "    return fuels,makes,duties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuels,makes,duties = get_classes(_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df = _24\n",
    "s = curr_df.groupby('zipcode').num_vehicles.sum()\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(24,5))\n",
    "ax[0].hist(s, bins=50)\n",
    "ax[0].set_title('All Fuels')\n",
    "ax[0].grid()\n",
    "ax[0].set_xlim(s.min(),s.max())\n",
    "\n",
    "ax[1].hist(s, bins=50, log=True)\n",
    "ax[1].set_title('All Fuels log')\n",
    "ax[1].grid()\n",
    "ax[1].set_xlim(s.min(),s.max())\n",
    "\n",
    "ax[2].hist(s, bins=100, cumulative=True)\n",
    "ax[2].set_title(\" cumulative\")\n",
    "ax[2].grid()\n",
    "ax[2].set_xlim(s.min(),s.max())\n",
    "\n",
    "#plt.savefig('figs/allfuelsfig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distributions of vehicle counts for zip codes for each fuel type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr = 2024\n",
    "for f in fuels:\n",
    "        s = curr_df[(curr_df['fuel'] == f) & (curr_df['zipcode'] != 'OOS')].groupby('zipcode').num_vehicles.sum()\n",
    "        fig, ax = plt.subplots(ncols=3, figsize=(24,5))\n",
    "        ax[0].hist(s, bins='auto')\n",
    "        ax[0].set_title(str(yr) +' '+ f)\n",
    "        ax[0].grid()\n",
    "        ax[0].set_xlim(s.min(),s.max())\n",
    "\n",
    "        ax[1].hist(s, bins='auto', log=True)\n",
    "        ax[1].set_title(str(yr) +' log '+ f)\n",
    "        ax[1].grid()\n",
    "        ax[1].set_xlim(s.min(),s.max())\n",
    "\n",
    "        ax[2].hist(s, bins=100, cumulative=True)\n",
    "        ax[2].set_title(str(yr) +' '+f + \" cumulative\")\n",
    "        ax[2].grid()\n",
    "        ax[2].set_xlim(s.min(),s.max())\n",
    "\n",
    "        plt.savefig(\"figs/\"+f)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering the Total number of Battery Electric Vehicles registered in each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf = pd.DataFrame(df.groupby(['fuel','date']).num_vehicles.sum()).reset_index()\n",
    "ts_fuels = tsdf.fuel.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting registration values total over the relevant years for each fuel type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_l = []\n",
    "for fuel in ts_fuels:\n",
    "    ts_l.append(tsdf[tsdf.fuel == fuel])\n",
    "#ts_be = tsdf[tsdf.fuel == 'Battery Electric']\n",
    "\n",
    "\n",
    "for fuel in ts_l:\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(6,4))\n",
    "    fuel_name = fuel.fuel.unique()[0]\n",
    "    ax.set_ylim(fuel.num_vehicles.min()/1.2,fuel.num_vehicles.max()*1.2)\n",
    "    ax.set_title(fuel_name)\n",
    "    ax.plot(fuel['date'], fuel['num_vehicles'],label=fuel_name)\n",
    "#ts_be.plot(ax=ax, kind='line', x='date', y='num_vehicles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data refers to a public user dataset for the geojson data regarding the geometry of us states; refer to the readme for details.\n",
    "# replace the source before using. \n",
    "url_usa = 'geojson data source'\n",
    "with requests.get(url_usa) as response:\n",
    "    us_geo = response.json()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = _24.groupby('zipcode').num_vehicles.sum().reset_index()\n",
    "geo_df = geo_df[geo_df.zipcode != 'OOS']\n",
    "geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import os\n",
    "#os.system('jupyter nbconvert --to html vft.ipynb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
